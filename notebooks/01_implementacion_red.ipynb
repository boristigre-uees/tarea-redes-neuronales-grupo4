{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d283655"
      },
      "source": [
        "## Definir Clase NeuralNetwork\n",
        "\n",
        "### Subtask:\n",
        "Definir la clase `NeuralNetwork` que implementa una red neuronal feedforward para regresión con NumPy, incluyendo inicialización, funciones de activación (relu, tanh, sigmoid), forward propagation, backward propagation y el método de entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "088278f5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    \"\"\"\n",
        "    Red neuronal feedforward (solo NumPy) para REGRESIÓN.\n",
        "    Caso Pitágoras: entrada [a, b] -> salida [c]\n",
        "    - >= 2 capas ocultas\n",
        "    - activación oculta: relu / tanh / sigmoid\n",
        "    - inicialización: He (relu) o Xavier (tanh/sigmoid)\n",
        "    - salida lineal + MSE\n",
        "    \"\"\"\n",
        "    def __init__(self, layers, activation='relu', seed=42):\n",
        "        assert len(layers) >= 4, \"Ej: [2, 32, 16, 1] (2 ocultas mínimo)\"\n",
        "        self.layers = layers\n",
        "        self.activation_name = activation.lower()\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        self.W, self.b = [], []\n",
        "        for l in range(len(layers) - 1):\n",
        "            fan_in, fan_out = layers[l], layers[l + 1]\n",
        "\n",
        "            if self.activation_name == 'relu':\n",
        "                w = np.random.randn(fan_in, fan_out) * np.sqrt(2.0 / fan_in)  # He\n",
        "            else:\n",
        "                w = np.random.randn(fan_in, fan_out) * np.sqrt(1.0 / fan_in)  # Xavier\n",
        "\n",
        "            b = np.zeros((1, fan_out))\n",
        "            self.W.append(w)\n",
        "            self.b.append(b)\n",
        "\n",
        "    def _act(self, Z):\n",
        "        if self.activation_name == 'relu':\n",
        "            return np.maximum(0, Z)\n",
        "        if self.activation_name == 'tanh':\n",
        "            return np.tanh(Z)\n",
        "        if self.activation_name == 'sigmoid':\n",
        "            return 1.0 / (1.0 + np.exp(-Z))\n",
        "        raise ValueError(\"activation: 'relu', 'tanh' o 'sigmoid'\")\n",
        "\n",
        "    def _act_deriv(self, Z, A):\n",
        "        if self.activation_name == 'relu':\n",
        "            return (Z > 0).astype(float)\n",
        "        if self.activation_name == 'tanh':\n",
        "            return 1.0 - A**2\n",
        "        if self.activation_name == 'sigmoid':\n",
        "            return A * (1.0 - A)\n",
        "        raise ValueError(\"activation: 'relu', 'tanh' o 'sigmoid'\")\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.cache = {\"A\": [X], \"Z\": []}\n",
        "        A = X\n",
        "        L = len(self.W)\n",
        "\n",
        "        for l in range(L):\n",
        "            Z = A @ self.W[l] + self.b[l]\n",
        "            self.cache[\"Z\"].append(Z)\n",
        "\n",
        "            if l < L - 1:\n",
        "                A = self._act(Z)   # ocultas\n",
        "            else:\n",
        "                A = Z              # salida lineal (regresión)\n",
        "\n",
        "            self.cache[\"A\"].append(A)\n",
        "\n",
        "        return A  # y_hat\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        y_hat = self.cache[\"A\"][-1]\n",
        "        m = X.shape[0]\n",
        "\n",
        "        dW = [None] * len(self.W)\n",
        "        db = [None] * len(self.b)\n",
        "\n",
        "        # MSE: dL/dy_hat\n",
        "        dA = (2.0 / m) * (y_hat - y)\n",
        "\n",
        "        for l in reversed(range(len(self.W))):\n",
        "            A_prev = self.cache[\"A\"][l]\n",
        "            Z_l = self.cache[\"Z\"][l]\n",
        "\n",
        "            if l == len(self.W) - 1:\n",
        "                dZ = dA  # salida lineal\n",
        "            else:\n",
        "                A_l = self.cache[\"A\"][l + 1]\n",
        "                dZ = dA * self._act_deriv(Z_l, A_l)\n",
        "\n",
        "            dW[l] = A_prev.T @ dZ\n",
        "            db[l] = np.sum(dZ, axis=0, keepdims=True)\n",
        "\n",
        "            dA = dZ @ self.W[l].T\n",
        "\n",
        "        return dW, db\n",
        "\n",
        "    def train(self, X, y, epochs=2000, learning_rate=0.01, verbose=200):\n",
        "        losses = []\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            y_hat = self.forward(X)\n",
        "            loss = np.mean((y_hat - y) ** 2)\n",
        "            losses.append(loss)\n",
        "\n",
        "            dW, db = self.backward(X, y)\n",
        "\n",
        "            for l in range(len(self.W)):\n",
        "                self.W[l] -= learning_rate * dW[l]\n",
        "                self.b[l] -= learning_rate * db[l]\n",
        "\n",
        "            if verbose and (epoch % verbose == 0 or epoch == 1 or epoch == epochs):\n",
        "                print(f\"Epoch {epoch}/{epochs} - MSE: {loss:.6f}\")\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c128e1e7"
      },
      "source": [
        "## Definir Funciones de Utilidad del Dataset\n",
        "\n",
        "### Subtask:\n",
        "Definir las funciones `make_pythagoras_data` para generar el dataset de Pitágoras y `standardize_fit` y `standardize_apply` para la normalización de los datos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23a06bc5"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining the utility functions for dataset generation and standardization. These functions (`make_pythagoras_data`, `standardize_fit`, `standardize_apply`) are already present in the initial code, so I will extract and present them in a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01964bd5",
        "outputId": "8e22ad64-4c7c-409d-b59a-eb49815460ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Utility functions for dataset generation and standardization defined.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def make_pythagoras_data(n=10000, a_max=20.0, b_max=20.0, seed=1):\n",
        "    \"\"\"\n",
        "    Genera pares (a,b) y su hipotenusa c = sqrt(a^2 + b^2)\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    a = rng.uniform(0.0, a_max, size=(n, 1))\n",
        "    b = rng.uniform(0.0, b_max, size=(n, 1))\n",
        "    c = np.sqrt(a**2 + b**2)\n",
        "    X = np.hstack([a, b])\n",
        "    y = c\n",
        "    return X, y\n",
        "\n",
        "def standardize_fit(X):\n",
        "    mu = X.mean(axis=0, keepdims=True)\n",
        "    sigma = X.std(axis=0, keepdims=True) + 1e-8\n",
        "    return mu, sigma\n",
        "\n",
        "def standardize_apply(X, mu, sigma):\n",
        "    return (X - mu) / sigma\n",
        "\n",
        "print(\"Utility functions for dataset generation and standardization defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "112b1b1c"
      },
      "source": [
        "## Generar y Preprocesar Datos\n",
        "\n",
        "### Subtask:\n",
        "Generar el dataset de Pitágoras, normalizar los datos de entrada (X) y dividir el dataset en conjuntos de entrenamiento y prueba.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c49e1790",
        "outputId": "090d2685-27d5-476f-835c-6d5b0103cc27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset generated, standardized, and split into training and testing sets.\n"
          ]
        }
      ],
      "source": [
        "X, y = make_pythagoras_data(n=5000, a_max=20, b_max=20, seed=7)\n",
        "\n",
        "mu, sigma = standardize_fit(X)\n",
        "Xn = standardize_apply(X, mu, sigma)\n",
        "\n",
        "idx = np.random.permutation(len(Xn))\n",
        "split = int(0.8 * len(Xn))\n",
        "tr, te = idx[:split], idx[split:]\n",
        "Xtr, ytr = Xn[tr], y[tr]\n",
        "Xte, yte = Xn[te], y[te]\n",
        "\n",
        "print(\"Dataset generated, standardized, and split into training and testing sets.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d057f01"
      },
      "source": [
        "## Inicializar y Entrenar la Red Neuronal\n",
        "\n",
        "### Subtask:\n",
        "Crear una instancia de la clase `NeuralNetwork` con la arquitectura y activación especificadas, y luego entrenarla usando el conjunto de datos de entrenamiento.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5c5dd07"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires initializing the `NeuralNetwork` with specific parameters and then training it using the provided training data. This can be achieved by creating an instance of the class and calling its `train` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e8ede4a",
        "outputId": "47bde1aa-c18d-4d00-d244-e7676cf172d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neural Network instance created.\n",
            "Epoch 1/5000 - MSE: 248.443506\n",
            "Epoch 500/5000 - MSE: 0.149880\n",
            "Epoch 1000/5000 - MSE: 0.097042\n",
            "Epoch 1500/5000 - MSE: 0.074537\n",
            "Epoch 2000/5000 - MSE: 0.042853\n",
            "Epoch 2500/5000 - MSE: 0.032761\n",
            "Epoch 3000/5000 - MSE: 0.024968\n",
            "Epoch 3500/5000 - MSE: 0.020643\n",
            "Epoch 4000/5000 - MSE: 0.017719\n",
            "Epoch 4500/5000 - MSE: 0.014610\n",
            "Epoch 5000/5000 - MSE: 0.014320\n",
            "Neural Network training complete.\n"
          ]
        }
      ],
      "source": [
        "nn = NeuralNetwork(layers=[2, 32, 16, 1], activation='relu', seed=42)\n",
        "print(\"Neural Network instance created.\")\n",
        "\n",
        "losses = nn.train(Xtr, ytr, epochs=5000, learning_rate=0.01, verbose=500)\n",
        "print(\"Neural Network training complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0c374c7"
      },
      "source": [
        "## Evaluar el Rendimiento del Modelo\n",
        "\n",
        "### Subtask:\n",
        "Realizar predicciones en el conjunto de prueba y calcular las métricas de rendimiento (MSE y MAE) para evaluar la precisión del modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2d20cd3",
        "outputId": "b9354bdc-4003-44a4-9b4f-7b918989a4bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test MSE: 0.014037 | Test MAE: 0.106365\n"
          ]
        }
      ],
      "source": [
        "pred_te = nn.predict(Xte)\n",
        "mse = np.mean((pred_te - yte) ** 2)\n",
        "mae = np.mean(np.abs(pred_te - yte))\n",
        "\n",
        "print(f\"\\nTest MSE: {mse:.6f} | Test MAE: {mae:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "148d31ee"
      },
      "source": [
        "## Probar Caso Específico\n",
        "\n",
        "### Subtask:\n",
        "Probar la red neuronal con un caso de entrada específico para predecir la hipotenusa y comparar el resultado con el valor real.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17f9a281",
        "outputId": "dd9e014c-cd80-45e5-bcf6-249a2c5f8dbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Pitágoras a=3.0, b=4.0 -> predicho: 5.06 | real: 5.0000\n"
          ]
        }
      ],
      "source": [
        "cateto_a = 3.0\n",
        "cateto_b = 4.0\n",
        "\n",
        "X_test = np.array([[cateto_a, cateto_b]])\n",
        "X_test_n = standardize_apply(X_test, mu, sigma)\n",
        "c_pred = nn.predict(X_test_n)[0, 0]\n",
        "c_real = np.sqrt(cateto_a**2 + cateto_b**2)\n",
        "\n",
        "print(f\"\\nPitágoras a={cateto_a}, b={cateto_b} -> predicho: {c_pred:.2f} | real: {c_real:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
